services:
  lifebuddy:
    build: 
      context: ..
      dockerfile: deployment/Dockerfile
    container_name: lifebuddy-app
    ports:
      - "8000:8000"   # FastAPI backend
      - "8501:8501"   # Streamlit frontend
    volumes:
      # Mount data directory for persistence
      - ../data:/app/data
      # Mount Downloads folder for automatic Apple Health export detection
      - ~/Downloads:/host/Downloads:ro  # Read-only access to Downloads
    environment:
      - LLM_PROVIDER=ollama
      - OLLAMA_BASE_URL=http://host.docker.internal:11434
      - OLLAMA_MODEL=llama3.2:3b
      - DATABASE_PATH=/app/data/lifebuddy.db
      # Uncomment and add your API keys to use external providers:
      # - OPENAI_API_KEY=your_openai_key_here
      # - ANTHROPIC_API_KEY=your_anthropic_key_here  
      # - GOOGLE_API_KEY=your_google_key_here
      # - AZURE_OPENAI_API_KEY=your_azure_key_here
      # - AZURE_OPENAI_ENDPOINT=your_azure_endpoint_here
      # - AZURE_OPENAI_DEPLOYMENT=your_azure_deployment_here
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health/status"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    # Resource limits (optimized for 8GB M1 Mac)
    deploy:
      resources:
        limits:
          memory: 1.5G    # Reduced from 2G to avoid memory pressure
          cpus: '1.5'     # Increased to utilize M1 performance cores
        reservations:
          memory: 512M    # Reduced guaranteed memory
          cpus: '0.5'     # Keep guaranteed CPU the same
    # Enable host networking for Ollama connection
    extra_hosts:
      - "host.docker.internal:host-gateway" 